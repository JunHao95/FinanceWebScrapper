Directory Structure:

└── ./
    └── FinanceWebScrapper
        ├── src
        │   ├── indicators
        │   │   ├── __init__.py
        │   │   └── technical_indicators.py
        │   ├── scrapers
        │   │   ├── __init__.py
        │   │   ├── api_scraper.py
        │   │   ├── base_scraper.py
        │   │   ├── finviz_scraper.py
        │   │   ├── google_scraper.py
        │   │   └── yahoo_scraper.py
        │   ├── utils
        │   │   ├── __init__.py
        │   │   ├── comparison_utils.py
        │   │   ├── data_formatter.py
        │   │   ├── display_formatter.py
        │   │   ├── email_utils.py
        │   │   └── request_handler.py
        │   ├── __init__.py
        │   └── config.py
        ├── tests
        │   └── test_yahoo_scraper.py
        ├── main.py
        └── setup.py



---
File: /FinanceWebScrapper/src/indicators/__init__.py
---




---
File: /FinanceWebScrapper/src/indicators/technical_indicators.py
---

"""
Technical Indicators Module for Stock Analysis
"""
import numpy as np
import pandas as pd
import requests
import os
import logging
from datetime import datetime, timedelta

class TechnicalIndicators:
    """
    Class to calculate and retrieve technical indicators for stocks
    """
    
    def __init__(self, api_key=None):
        """
        Initialize the technical indicators module
        
        Args:
            api_key (str): Alpha Vantage API key. If None, will try to get from ALPHA_VANTAGE_API_KEY environment variable
        """
        self.api_key = api_key or os.environ.get("ALPHA_VANTAGE_API_KEY")
        self.logger = logging.getLogger(self.__class__.__name__)
        
        if not self.api_key:
            self.logger.warning("Alpha Vantage API key not provided. Set ALPHA_VANTAGE_API_KEY environment variable.")
    
    def get_historical_data(self, ticker, days=100):
        """
        Retrieve historical price data for a ticker
        
        Args:
            ticker (str): Stock ticker symbol
            days (int): Number of days of historical data to fetch
            
        Returns:
            pandas.DataFrame: DataFrame containing the historical data
        """
        if not self.api_key:
            self.logger.error("Alpha Vantage API key not available. Cannot fetch historical data.")
            return pd.DataFrame()
        
        try:
            # Get daily price data from Alpha Vantage
            url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&outputsize=full&apikey={self.api_key}"
            
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if "Error Message" in data:
                self.logger.error(f"API Error: {data['Error Message']}")
                return pd.DataFrame()
                
            if "Time Series (Daily)" not in data:
                self.logger.error("No time series data returned from API")
                return pd.DataFrame()
            
            # Convert to DataFrame
            time_series = data["Time Series (Daily)"]
            df = pd.DataFrame.from_dict(time_series, orient='index')
            
            # Convert index to datetime
            df.index = pd.to_datetime(df.index)
            
            # Convert columns to numeric
            for col in df.columns:
                df[col] = pd.to_numeric(df[col])
            
            # Rename columns
            df.rename(columns={
                '1. open': 'open',
                '2. high': 'high',
                '3. low': 'low',
                '4. close': 'close',
                '5. volume': 'volume'
            }, inplace=True)
            
            # Sort by date (most recent first)
            df.sort_index(ascending=False, inplace=True)
            
            # Limit to requested number of days
            df = df.head(days)
            
            # Handle NaN values - fill with preceding values
            df.fillna(method='ffill', inplace=True)
            
            # Check if we have enough data
            if len(df) < 5:
                self.logger.warning(f"Insufficient historical data for {ticker}: only {len(df)} days available")
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error fetching historical data: {str(e)}")
            return pd.DataFrame()
    
    def calculate_bollinger_bands(self, df, window=20, num_std=2):
        """
        Calculate Bollinger Bands for a given DataFrame
        
        Args:
            df (pandas.DataFrame): DataFrame with price data
            window (int): Window size for moving average
            num_std (int): Number of standard deviations for bands
            
        Returns:
            dict: Dictionary with Bollinger Bands values
        """
        if df.empty:
            return {}
            
        try:
            # Make sure we have enough data
            if len(df) < window:
                self.logger.warning(f"Not enough data for Bollinger Bands calculation: {len(df)} < {window}")
                return {}
                
            # Calculate middle band (SMA)
            middle_band = df['close'].rolling(window=window).mean()
            
            # Calculate standard deviation
            std = df['close'].rolling(window=window).std()
            
            # Calculate upper and lower bands
            upper_band = middle_band + (std * num_std)
            lower_band = middle_band - (std * num_std)
            
            # Get the most recent values
            current_middle = middle_band.iloc[0]
            current_upper = upper_band.iloc[0]
            current_lower = lower_band.iloc[0]
            current_close = df['close'].iloc[0]
            
            # Calculate band width and %B
            band_width = (current_upper - current_lower) / current_middle * 100
            
            # Avoid division by zero
            if current_upper == current_lower:
                percent_b = 50.0
            else:
                percent_b = (current_close - current_lower) / (current_upper - current_lower) * 100
            
            # Determine indicator signal
            if current_close > current_upper:
                signal = "Overbought"
            elif current_close < current_lower:
                signal = "Oversold"
            else:
                signal = "Neutral"
            
            return {
                "BB Middle Band": round(current_middle, 2),
                "BB Upper Band": round(current_upper, 2),
                "BB Lower Band": round(current_lower, 2),
                "BB Width (%)": round(band_width, 2),
                "BB %B": round(percent_b, 2),
                "BB Signal": signal
            }
            
        except Exception as e:
            self.logger.error(f"Error calculating Bollinger Bands: {str(e)}")
            return {}
    
    def calculate_moving_averages(self, df):
        """
        Calculate various moving averages
        
        Args:
            df (pandas.DataFrame): DataFrame with price data
            
        Returns:
            dict: Dictionary with moving average values
        """
        if df.empty:
            return {}
            
        try:
            # Calculate different moving averages
            ma_windows = [10, 20, 50, 100, 200]
            mas = {}
            
            for window in ma_windows:
                if len(df) >= window:
                    ma = df['close'].rolling(window=window).mean()
                    mas[f"MA{window}"] = round(ma.iloc[0], 2)
            
            # Calculate exponential moving averages
            ema_windows = [12, 26, 50, 200]
            emas = {}
            
            for window in ema_windows:
                if len(df) >= window:
                    ema = df['close'].ewm(span=window, adjust=False).mean()
                    emas[f"EMA{window}"] = round(ema.iloc[0], 2)
            
            # Calculate MACD
            if len(df) >= 26:
                ema12 = df['close'].ewm(span=12, adjust=False).mean()
                ema26 = df['close'].ewm(span=26, adjust=False).mean()
                macd = ema12 - ema26
                signal = macd.ewm(span=9, adjust=False).mean()
                macd_hist = macd - signal
                
                macd_data = {
                    "MACD Line": round(macd.iloc[0], 2),
                    "MACD Signal": round(signal.iloc[0], 2),
                    "MACD Histogram": round(macd_hist.iloc[0], 2)
                }
            else:
                macd_data = {}
            
            # Determine crossover signals
            signals = {}
            current_close = df['close'].iloc[0]
            
            for window in ma_windows:
                if f"MA{window}" in mas:
                    if current_close > mas[f"MA{window}"]:
                        signals[f"MA{window} Signal"] = "Bullish (Price > MA)"
                    else:
                        signals[f"MA{window} Signal"] = "Bearish (Price < MA)"
            
            # Add all data to result
            result = {**mas, **emas, **macd_data, **signals}
            
            # Add current price for reference
            result["Current Price"] = round(current_close, 2)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating Moving Averages: {str(e)}")
            return {}
    
    def calculate_rsi(self, df, window=14):
        """
        Calculate Relative Strength Index (RSI)
        
        Args:
            df (pandas.DataFrame): DataFrame with price data
            window (int): Window size for RSI calculation
            
        Returns:
            dict: Dictionary with RSI values
        """
        if df.empty or len(df) < window + 1:
            return {}
            
        try:
            # Calculate price differences
            delta = df['close'].diff()
            
            # Create copies for gains and losses
            gain = delta.copy()
            loss = delta.copy()
            
            # Separate gains and losses
            gain[gain < 0] = 0
            loss[loss > 0] = 0
            loss = abs(loss)
            
            # Calculate average gain and loss
            avg_gain = gain.rolling(window=window).mean()
            avg_loss = loss.rolling(window=window).mean()
            
            # Handle division by zero
            avg_loss = avg_loss.replace(0, 0.00001)
            
            # Calculate RS and RSI
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            
            # Get current RSI value
            current_rsi = round(rsi.iloc[0], 2)
            
            # Determine RSI signal
            if current_rsi > 70:
                signal = "Overbought"
            elif current_rsi < 30:
                signal = "Oversold"
            else:
                signal = "Neutral"
            
            return {
                "RSI (14)": current_rsi,
                "RSI Signal": signal
            }
            
        except Exception as e:
            self.logger.error(f"Error calculating RSI: {str(e)}")
            return {}
    
    def calculate_volume_indicators(self, df):
        """
        Calculate volume-based indicators
        
        Args:
            df (pandas.DataFrame): DataFrame with price data and volume
            
        Returns:
            dict: Dictionary with volume indicator values
        """
        if df.empty:
            return {}
            
        try:
            # Ensure volume data is not NaN
            if df['volume'].isnull().all():
                self.logger.warning("No volume data available")
                return {"Volume Data": "Not Available"}
            
            # Fill any NaN values with 0
            df['volume'] = df['volume'].fillna(0)
            
            # Calculate volume moving average
            volume_ma_windows = [10, 20, 50]
            volume_mas = {}
            
            for window in volume_ma_windows:
                if len(df) >= window:
                    volume_ma = df['volume'].rolling(window=window).mean()
                    # Safely convert to int with NaN handling
                    if not np.isnan(volume_ma.iloc[0]):
                        volume_mas[f"Volume MA{window}"] = int(volume_ma.iloc[0])
                    else:
                        volume_mas[f"Volume MA{window}"] = 0
            
            # Current volume - safely get as int
            try:
                current_volume = int(df['volume'].iloc[0])
            except (TypeError, ValueError):
                current_volume = 0
            
            # Determine volume signals
            signals = {}
            for window in volume_ma_windows:
                if f"Volume MA{window}" in volume_mas:
                    if current_volume > volume_mas[f"Volume MA{window}"]:
                        signals[f"Volume MA{window} Signal"] = "Above Average"
                    else:
                        signals[f"Volume MA{window} Signal"] = "Below Average"
            
            # On-Balance Volume (OBV)
            # Handle NaN values in volume
            safe_volume = df['volume'].fillna(0).astype(int)
            
            # Start with zero OBV and add/subtract volume based on price movement
            obv = [0]
            for i in range(1, len(df)):
                if df['close'].iloc[i] > df['close'].iloc[i-1]:
                    obv.append(obv[-1] + safe_volume.iloc[i])
                elif df['close'].iloc[i] < df['close'].iloc[i-1]:
                    obv.append(obv[-1] - safe_volume.iloc[i])
                else:
                    obv.append(obv[-1])
            
            # Create a Series with the OBV values
            obv_series = pd.Series(obv, index=df.index)
            
            # Calculate OBV moving average
            obv_ma = obv_series.rolling(window=20).mean()
            
            # Determine OBV trend
            obv_trend = "Neutral"
            if len(obv_series) > 5:
                if obv_series.iloc[0] > obv_series.iloc[4]:
                    obv_trend = "Bullish"
                elif obv_series.iloc[0] < obv_series.iloc[4]:
                    obv_trend = "Bearish"
            
            # Add to results
            volume_data = {
                "Current Volume": current_volume,
                "OBV Trend": obv_trend,
                **volume_mas,
                **signals
            }
            
            return volume_data
            
        except Exception as e:
            self.logger.error(f"Error calculating Volume Indicators: {str(e)}")
            return {"Volume Indicators Error": str(e)}
    
    def get_all_indicators(self, ticker):
        """
        Calculate all technical indicators for a ticker
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary with all technical indicators
        """
        # Get historical data
        df = self.get_historical_data(ticker)
        
        if df.empty:
            return {"error": "Could not retrieve historical data"}
        
        # Calculate all indicators
        results = {"Ticker": ticker, "Last Updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
        
        # Try each indicator separately and gracefully handle failures
        try:
            bollinger_bands = self.calculate_bollinger_bands(df)
            results.update(bollinger_bands)
        except Exception as e:
            self.logger.error(f"Failed to calculate Bollinger Bands: {str(e)}")
            results["Bollinger Bands"] = "Calculation Error"
        
        try:
            moving_averages = self.calculate_moving_averages(df)
            results.update(moving_averages)
        except Exception as e:
            self.logger.error(f"Failed to calculate Moving Averages: {str(e)}")
            results["Moving Averages"] = "Calculation Error"
        
        try:
            rsi = self.calculate_rsi(df)
            results.update(rsi)
        except Exception as e:
            self.logger.error(f"Failed to calculate RSI: {str(e)}")
            results["RSI"] = "Calculation Error"
        
        try:
            volume_indicators = self.calculate_volume_indicators(df)
            results.update(volume_indicators)
        except Exception as e:
            self.logger.error(f"Failed to calculate Volume Indicators: {str(e)}")
            results["Volume Indicators"] = "Calculation Error"
        
        return results


---
File: /FinanceWebScrapper/src/scrapers/__init__.py
---




---
File: /FinanceWebScrapper/src/scrapers/api_scraper.py
---

"""
Financial API scraper module
"""
import os
import requests
import time
import logging
from .base_scraper import BaseScraper

class AlphaVantageAPIScraper(BaseScraper):
    """Scraper for Alpha Vantage Financial API"""
    
    def __init__(self, api_key=None, delay=1):
        """
        Initialize the Alpha Vantage API scraper
        
        Args:
            api_key (str): Alpha Vantage API key. If None, will try to get from ALPHA_VANTAGE_API_KEY environment variable
            delay (int): Delay in seconds between requests to avoid rate limiting
        """
        super().__init__(delay=delay)
        # Get API key from environment variable if not provided
        self.api_key = api_key or os.environ.get("ALPHA_VANTAGE_API_KEY")
        if not self.api_key:
            self.logger.warning("Alpha Vantage API key not provided. Set ALPHA_VANTAGE_API_KEY environment variable.")
        
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def _scrape_data(self, ticker):
        """
        Get financial data from Alpha Vantage API
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary containing scraped data
        """
        if not self.api_key:
            self.logger.error("Alpha Vantage API key not available. Skipping API data source.")
            return {"error": "Alpha Vantage API key not available"}
        
        data = {}
        
        # Get company overview (contains most fundamental data)
        try:
            self.logger.info(f"Fetching company overview from Alpha Vantage for {ticker}")
            overview_url = f"https://www.alphavantage.co/query?function=OVERVIEW&symbol={ticker}&apikey={self.api_key}"
            
            response = requests.get(overview_url, timeout=10)
            response.raise_for_status()
            
            result = response.json()
            
            # Check if we have actual data or an error
            if "Symbol" not in result:
                self.logger.warning(f"No overview data found for {ticker} on Alpha Vantage")
                return {}
            
            # Map Alpha Vantage fields to our standard format
            field_mapping = {
                "PERatio": "P/E Ratio (AlphaVantage)",
                "PriceToBookRatio": "P/B Ratio (AlphaVantage)",
                "PriceToSalesRatioTTM": "P/S Ratio (AlphaVantage)",
                "EPS": "EPS (AlphaVantage)",
                "ForwardPE": "Forward P/E (AlphaVantage)",
                "PEGRatio": "PEG Ratio (AlphaVantage)",
                "EVToEBITDA": "EV/EBITDA (AlphaVantage)",
                "ReturnOnEquityTTM": "ROE (AlphaVantage)",
                "ReturnOnAssetsTTM": "ROA (AlphaVantage)",
                "ProfitMargin": "Profit Margin (AlphaVantage)",
                "OperatingMarginTTM": "Operating Margin (AlphaVantage)",
                "DividendYield": "Dividend Yield (AlphaVantage)",
                "BookValue": "Book Value (AlphaVantage)",
                "TrailingPE": "Trailing P/E (AlphaVantage)",
                "Beta": "Beta (AlphaVantage)"
            }
            
            for av_field, our_field in field_mapping.items():
                if av_field in result and result[av_field]:
                    # Ensure values are numeric where possible
                    try:
                        value = float(result[av_field])
                        # Format percentages in a recognizable way
                        if "Margin" in our_field or "ROA" in our_field or "ROE" in our_field or "Yield" in our_field:
                            data[our_field] = f"{value:.2f}%"
                        else:
                            data[our_field] = f"{value:.2f}"
                    except (ValueError, TypeError):
                        data[our_field] = result[av_field]
            
            # Add additional metrics for context
            if "Name" in result:
                data["Company Name (AlphaVantage)"] = result["Name"]
            if "Sector" in result:
                data["Sector (AlphaVantage)"] = result["Sector"]
            if "Industry" in result:
                data["Industry (AlphaVantage)"] = result["Industry"]
            
            # Wait before next API call (if needed)
            time.sleep(self.delay)
            
            # Get additional cash flow data
            try:
                self.logger.info(f"Fetching cash flow data from Alpha Vantage for {ticker}")
                cash_flow_url = f"https://www.alphavantage.co/query?function=CASH_FLOW&symbol={ticker}&apikey={self.api_key}"
                
                cf_response = requests.get(cash_flow_url, timeout=10)
                cf_response.raise_for_status()
                
                cf_result = cf_response.json()
                
                if "annualReports" in cf_result and cf_result["annualReports"]:
                    latest_report = cf_result["annualReports"][0]
                    
                    if "operatingCashflow" in latest_report and latest_report["operatingCashflow"]:
                        data["Operating Cash Flow (AlphaVantage)"] = latest_report["operatingCashflow"]
                    
                    if "capitalExpenditures" in latest_report and latest_report["capitalExpenditures"]:
                        data["Capital Expenditures (AlphaVantage)"] = latest_report["capitalExpenditures"]
                        
                        # Calculate Free Cash Flow
                        try:
                            ocf = float(latest_report["operatingCashflow"])
                            capex = float(latest_report["capitalExpenditures"])
                            fcf = ocf - abs(capex)  # capex is usually negative
                            data["Free Cash Flow (AlphaVantage)"] = f"{fcf:.2f}"
                        except (ValueError, TypeError):
                            pass
            
            except Exception as e:
                self.logger.warning(f"Error fetching cash flow data for {ticker}: {str(e)}")
            
            return data
                
        except Exception as e:
            self.logger.error(f"Error scraping Alpha Vantage API for {ticker}: {str(e)}")
            return {"error": f"Error scraping Alpha Vantage API: {str(e)}"}


class FinhubAPIScraper(BaseScraper):
    """Scraper for Finhub Financial API"""
    
    def __init__(self, api_key=None, delay=1):
        """
        Initialize the Finhub API scraper
        
        Args:
            api_key (str): Finhub API key. If None, will try to get from FINHUB_API_KEY environment variable
            delay (int): Delay in seconds between requests to avoid rate limiting
        """
        super().__init__(delay=delay)
        # Get API key from environment variable if not provided
        self.api_key = api_key or os.environ.get("FINHUB_API_KEY")
        if not self.api_key:
            self.logger.warning("Finhub API key not provided. Set FINHUB_API_KEY environment variable.")
        
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def _scrape_data(self, ticker):
        """
        Get financial data from Finhub API
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary containing scraped data
        """
        if not self.api_key:
            self.logger.error("Finhub API key not available. Skipping API data source.")
            return {"error": "Finhub API key not available"}
        
        data = {}
        
        # Get basic metrics
        try:
            self.logger.info(f"Fetching metrics from Finhub for {ticker}")
            metrics_url = f"https://finnhub.io/api/v1/stock/metric?symbol={ticker}&metric=all&token={self.api_key}"
            
            response = requests.get(metrics_url, timeout=10)
            response.raise_for_status()
            
            result = response.json()
            
            if "metric" not in result:
                self.logger.warning(f"No metric data found for {ticker} on Finhub")
                return {}
            
            metrics = result["metric"]
            
            # Map Finhub fields to our standard format
            field_mapping = {
                "currentPE": "P/E Ratio (Finhub)",
                "pbLTM": "P/B Ratio (Finhub)",
                "psLTM": "P/S Ratio (Finhub)",
                "epsBasicExclExtraItemsAnnual": "EPS (Finhub)",
                "peForward": "Forward P/E (Finhub)",
                "pegTTM": "PEG Ratio (Finhub)",
                "evToEbitdaTTM": "EV/EBITDA (Finhub)",
                "roeTTM": "ROE (Finhub)",
                "roaTTM": "ROA (Finhub)",
                "roicTTM": "ROIC (Finhub)",
                "netProfitMarginTTM": "Profit Margin (Finhub)",
                "operatingMarginTTM": "Operating Margin (Finhub)",
                "dividendYieldIndicatedAnnual": "Dividend Yield (Finhub)",
                "bookValuePerShareQuarterly": "Book Value (Finhub)",
                "beta": "Beta (Finhub)"
            }
            
            for fh_field, our_field in field_mapping.items():
                if fh_field in metrics and metrics[fh_field] is not None:
                    # Format percentages in a recognizable way
                    if "Margin" in our_field or "ROA" in our_field or "ROE" in our_field or "ROIC" in our_field or "Yield" in our_field:
                        data[our_field] = f"{metrics[fh_field]:.2f}%"
                    else:
                        data[our_field] = f"{metrics[fh_field]:.2f}"
            
            # Get company profile for additional context
            try:
                profile_url = f"https://finnhub.io/api/v1/stock/profile2?symbol={ticker}&token={self.api_key}"
                
                profile_response = requests.get(profile_url, timeout=10)
                profile_response.raise_for_status()
                
                profile = profile_response.json()
                
                if profile:
                    if "name" in profile:
                        data["Company Name (Finhub)"] = profile["name"]
                    if "finnhubIndustry" in profile:
                        data["Industry (Finhub)"] = profile["finnhubIndustry"]
                    if "marketCapitalization" in profile:
                        data["Market Cap (Finhub)"] = f"{profile['marketCapitalization']:.2f}B"
            
            except Exception as e:
                self.logger.warning(f"Error fetching company profile for {ticker}: {str(e)}")
            
            return data
                
        except Exception as e:
            self.logger.error(f"Error scraping Finhub API for {ticker}: {str(e)}")
            return {"error": f"Error scraping Finhub API: {str(e)}"}


---
File: /FinanceWebScrapper/src/scrapers/base_scraper.py
---

"""
Base Scraper module that defines the interface for all scraper classes
"""
import time
import logging
from abc import ABC, abstractmethod

from ..utils.request_handler import make_request

class BaseScraper(ABC):
    """Base class for all scrapers that defines common functionality"""
    
    def __init__(self, delay=1):
        """
        Initialize the scraper
        
        Args:
            delay (int): Delay in seconds between requests to avoid rate limiting
        """
        self.delay = delay
        self.logger = logging.getLogger(self.__class__.__name__)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                         '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def get_data(self, ticker):
        """
        Get data for the given ticker
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary containing scraped data
        """
        # Add delay to avoid being blocked
        time.sleep(self.delay)
        
        # Implement scraping logic in the derived class
        try:
            return self._scrape_data(ticker)
        except Exception as e:
            self.logger.error(f"Error scraping data for {ticker}: {str(e)}")
            return {"error": f"Error scraping data: {str(e)}"}
    
    @abstractmethod
    def _scrape_data(self, ticker):
        """
        Implement the actual scraping logic
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary containing scraped data
        """
        pass


---
File: /FinanceWebScrapper/src/scrapers/finviz_scraper.py
---

"""
Finviz scraper module
"""
from bs4 import BeautifulSoup

from .base_scraper import BaseScraper
from ..utils.request_handler import make_request

class FinvizScraper(BaseScraper):
    """Scraper for Finviz"""
    
    def _scrape_data(self, ticker):
        """
        Scrape financial data from Finviz
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary containing scraped data
        """
        url = f"https://finviz.com/quote.ashx?t={ticker}"
        
        response = make_request(url, headers=self.headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Initialize data dictionary
        data = {}
        
        # Find the snapshot table
        snapshot_table = soup.find('table', class_='snapshot-table2')
        if snapshot_table:
            rows = snapshot_table.find_all('tr')
            for row in rows:
                cells = row.find_all('td')
                for i in range(0, len(cells), 2):
                    if i + 1 < len(cells):
                        header = cells[i].text.strip()
                        value = cells[i+1].text.strip()
                        
                        # Basic valuation metrics
                        if header == "P/E":
                            data["P/E Ratio (Finviz)"] = value
                        elif header == "P/B":
                            data["P/B Ratio (Finviz)"] = value
                        elif header == "P/S":
                            data["P/S Ratio (Finviz)"] = value
                        elif header == "Forward P/E":
                            data["Forward P/E (Finviz)"] = value
                        
                        # Additional metrics
                        elif header == "PEG":
                            data["PEG Ratio (Finviz)"] = value
                        elif header == "EV/EBITDA":
                            data["EV/EBITDA (Finviz)"] = value
                        elif header == "ROE":
                            data["ROE (Finviz)"] = value
                        elif header == "ROA":
                            data["ROA (Finviz)"] = value
                        elif header == "ROI":
                            data["ROIC (Finviz)"] = value
                        elif header == "EPS (ttm)":
                            data["EPS (Finviz)"] = value
                        elif header == "EPS next Y":
                            data["EPS Next Year (Finviz)"] = value
                        elif header == "EPS growth this year":
                            data["EPS Growth This Year (Finviz)"] = value
                        elif header == "EPS growth next year":
                            data["EPS Growth Next Year (Finviz)"] = value
                        elif header == "EPS growth next 5Y":
                            data["EPS Growth Next 5Y (Finviz)"] = value
                        elif header == "EPS growth qtr over qtr":
                            data["EPS Growth QoQ (Finviz)"] = value
                        elif header == "Profit Margin":
                            data["Profit Margin (Finviz)"] = value
                        elif header == "Oper. Margin":
                            data["Operating Margin (Finviz)"] = value
        else:
            self.logger.warning(f"Could not find snapshot table for {ticker} on Finviz")
        
        # Add source metadata
        if data:
            self.logger.info(f"Successfully scraped Finviz data for {ticker}")
        else:
            self.logger.warning(f"No data found for {ticker} on Finviz")
            
        return data


---
File: /FinanceWebScrapper/src/scrapers/google_scraper.py
---

"""
Google Finance scraper module
"""
import re
from bs4 import BeautifulSoup

from .base_scraper import BaseScraper
from ..utils.request_handler import make_request

class GoogleFinanceScraper(BaseScraper):
    """Scraper for Google Finance"""
    
    def _scrape_data(self, ticker):
        """
        Scrape financial data from Google Finance
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary containing scraped data
        """
        # Google Finance URL format may vary by region/market
        # Try to handle different formats
        urls = [
            f"https://www.google.com/finance/quote/{ticker}:NASDAQ",
            f"https://www.google.com/finance/quote/{ticker}:NYSE",
            f"https://www.google.com/finance/quote/{ticker}"
        ]
        
        # Initialize data dictionary
        data = {}
        
        # Try each possible URL
        for url in urls:
            try:
                self.logger.info(f"Trying Google Finance URL: {url}")
                response = make_request(url, headers=self.headers)
                
                # If we get here, the URL worked
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Look for key metrics in the About section
                about_section = None
                sections = soup.find_all('div', class_='bLLb2d')
                for section in sections:
                    if "About" in section.text:
                        about_section = section
                        break
                
                if about_section:
                    # Google Finance structure is different from other sources
                    # It has a grid of metrics with labels and values
                    metrics = soup.find_all('div', class_='P6K39c')
                    for metric in metrics:
                        label_div = metric.find('div')
                        if not label_div:
                            continue
                            
                        label = label_div.text.strip().lower()
                        value_div = metric.find('div', class_='YMlKec')
                        
                        if value_div:
                            value = value_div.text.strip()
                            
                            # Basic metrics
                            if "p/e ratio" in label:
                                data["P/E Ratio (Google)"] = value
                            elif "price-to-book" in label or "p/b ratio" in label:
                                data["P/B Ratio (Google)"] = value
                            elif "price-to-sales" in label or "p/s ratio" in label:
                                data["P/S Ratio (Google)"] = value
                            
                            # Additional metrics
                            elif "eps" in label or "earnings per share" in label:
                                data["EPS (Google)"] = value
                            elif "roe" in label or "return on equity" in label:
                                data["ROE (Google)"] = value
                            elif "roic" in label or "return on invested capital" in label:
                                data["ROIC (Google)"] = value
                            elif "ev/ebitda" in label or "enterprise value to ebitda" in label:
                                data["EV/EBITDA (Google)"] = value
                            elif "peg" in label:
                                data["PEG Ratio (Google)"] = value
                
                # Look for EPS in description text or elsewhere on the page
                desc_div = soup.find('div', class_='bLLb2d')
                if desc_div:
                    desc_text = desc_div.text
                    eps_match = re.search(r'EPS.*?(\$[\d.]+|\d+\.\d+)', desc_text)
                    if eps_match and "EPS (Google)" not in data:
                        data["EPS (Google)"] = eps_match.group(1)
                
                # If we found at least one metric, break the loop
                if data:
                    break
                    
            except Exception as e:
                self.logger.warning(f"Failed to fetch Google Finance data from {url}: {str(e)}")
                continue
        
        # Add source metadata if we found data
        if data:
            self.logger.info(f"Successfully scraped Google Finance data for {ticker}")
        else:
            self.logger.warning(f"Could not find any metrics for {ticker} on Google Finance")
            
        return data


---
File: /FinanceWebScrapper/src/scrapers/yahoo_scraper.py
---

"""
Yahoo Finance scraper module
"""
from bs4 import BeautifulSoup

from .base_scraper import BaseScraper
from ..utils.request_handler import make_request

class YahooFinanceScraper(BaseScraper):
    """Scraper for Yahoo Finance"""
    
    def _scrape_data(self, ticker):
        """
        Scrape financial data from Yahoo Finance
        
        Args:
            ticker (str): Stock ticker symbol
            
        Returns:
            dict: Dictionary containing scraped data
        """
        # Statistics page contains most valuation metrics
        statistics_url = f"https://finance.yahoo.com/quote/{ticker}/key-statistics"
        
        # Analysis page contains EPS and growth estimates
        analysis_url = f"https://finance.yahoo.com/quote/{ticker}/analysis"
        
        # Initialize data dictionary
        data = {}
        
        # Scrape statistics page
        try:
            self.logger.info(f"Fetching statistics from Yahoo Finance for {ticker}")
            response = make_request(statistics_url, headers=self.headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Parse valuation ratios from tables
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= 2:
                        header = cols[0].text.strip()
                        value = cols[1].text.strip()
                        
                        # Basic valuation metrics
                        if "P/E Ratio" in header:
                            data["P/E Ratio (Yahoo)"] = value
                        elif "Price/Book" in header:
                            data["P/B Ratio (Yahoo)"] = value
                        elif "Price/Sales" in header:
                            data["P/S Ratio (Yahoo)"] = value
                        elif "Forward P/E" in header:
                            data["Forward P/E (Yahoo)"] = value
                        
                        # Additional metrics
                        elif "PEG Ratio" in header:
                            data["PEG Ratio (Yahoo)"] = value
                        elif "Enterprise Value/EBITDA" in header or "EV/EBITDA" in header:
                            data["EV/EBITDA (Yahoo)"] = value
                        elif "Return on Equity" in header or "ROE" in header:
                            data["ROE (Yahoo)"] = value
                        elif "Return on Assets" in header or "ROA" in header:
                            data["ROA (Yahoo)"] = value
                        elif "Profit Margin" in header:
                            data["Profit Margin (Yahoo)"] = value
                        elif "Operating Margin" in header:
                            data["Operating Margin (Yahoo)"] = value
                        elif "Diluted EPS" in header:
                            data["EPS (Yahoo)"] = value
                        elif "Return on Investment" in header or "Return on Capital" in header:
                            data["ROIC (Yahoo)"] = value
            
            # Scrape analysis page for additional EPS data
            try:
                analysis_response = make_request(analysis_url, headers=self.headers)
                analysis_soup = BeautifulSoup(analysis_response.text, 'html.parser')
                
                # Looking for EPS estimates and growth rates
                tables = analysis_soup.find_all('table')
                for table in tables:
                    table_text = table.text.lower()
                    if "earnings estimate" in table_text:
                        rows = table.find_all('tr')
                        for row in rows:
                            cells = row.find_all('td')
                            if len(cells) >= 2:
                                header = cells[0].text.strip().lower()
                                if "current year" in header and len(cells) > 1:
                                    data["EPS Estimate Current Year (Yahoo)"] = cells[1].text.strip()
                                elif "next year" in header and len(cells) > 1:
                                    data["EPS Estimate Next Year (Yahoo)"] = cells[1].text.strip()
            
            except Exception as e:
                self.logger.warning(f"Error scraping Yahoo Finance analysis page: {str(e)}")
        
        except Exception as e:
            self.logger.error(f"Error scraping Yahoo Finance statistics page: {str(e)}")
        
        # Add source metadata
        if data:
            self.logger.info(f"Successfully scraped Yahoo Finance data for {ticker}")
        else:
            self.logger.warning(f"No data found for {ticker} on Yahoo Finance")
            
        return data


---
File: /FinanceWebScrapper/src/utils/__init__.py
---




---
File: /FinanceWebScrapper/src/utils/comparison_utils.py
---

"""
Utilities for comparing and analyzing multiple stocks
"""
import pandas as pd
import numpy as np
import os
from datetime import datetime
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

def normalize_data(data_dict):
    """
    Normalize data from different sources into a consistent format
    
    Args:
        data_dict (dict): Dictionary with ticker symbols as keys and data dictionaries as values
    
    Returns:
        dict: Normalized data dictionary
    """
    normalized = {}
    
    # Define mappings for common metrics that might have different labels
    metric_mappings = {
        'P/E Ratio': ['P/E Ratio', 'PE Ratio', 'P/E', 'Price to Earnings'],
        'Forward P/E': ['Forward P/E', 'Fwd P/E', 'Forward PE'],
        'PEG Ratio': ['PEG Ratio', 'PEG', 'PE Growth'],
        'P/B Ratio': ['P/B Ratio', 'P/B', 'Price to Book'],
        'P/S Ratio': ['P/S Ratio', 'P/S', 'Price to Sales'],
        'EV/EBITDA': ['EV/EBITDA', 'Enterprise Value/EBITDA', 'EV to EBITDA'],
        'ROE': ['ROE', 'Return on Equity'],
        'ROA': ['ROA', 'Return on Assets'],
        'ROIC': ['ROIC', 'Return on Invested Capital', 'Return on Capital'],
        'Profit Margin': ['Profit Margin', 'Net Margin'],
        'Operating Margin': ['Operating Margin', 'EBIT Margin'],
        'EPS': ['EPS', 'Earnings Per Share'],
        'Current Price': ['Current Price', 'Price', 'Last Price'],
        'RSI': ['RSI (14)', 'RSI', 'Relative Strength Index'],
        'Beta': ['Beta']
    }
    
    for ticker, stock_data in data_dict.items():
        normalized[ticker] = {
            'Ticker': ticker,
            'Data Timestamp': stock_data.get('Data Timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        }
        
        # Process each metric type
        for normalized_name, possible_names in metric_mappings.items():
            for key in stock_data:
                if any(name in key for name in possible_names):
                    # Try to convert string values to numeric
                    try:
                        # Remove % and convert to float
                        value = stock_data[key]
                        if isinstance(value, str):
                            if '%' in value:
                                value = value.replace('%', '')
                            value = float(value)
                        normalized[ticker][normalized_name] = value
                        break
                    except (ValueError, TypeError):
                        normalized[ticker][normalized_name] = stock_data[key]
                        break
    
    return normalized

def create_comparison_dataframe(data_dict):
    """
    Create a DataFrame for comparing stocks
    
    Args:
        data_dict (dict): Dictionary with ticker symbols as keys and data dictionaries as values
    
    Returns:
        pandas.DataFrame: DataFrame with comparison data
    """
    # Normalize the data first
    normalized = normalize_data(data_dict)
    
    # Convert to DataFrame
    rows = []
    for ticker, data in normalized.items():
        rows.append(data)
    
    df = pd.DataFrame(rows)
    
    # Set ticker as index
    if 'Ticker' in df.columns:
        df.set_index('Ticker', inplace=True)
    
    return df

def create_metric_charts(df, columns, title, pdf):
    """
    Create charts for a set of metrics and add to PDF
    
    Args:
        df (pandas.DataFrame): DataFrame with comparison data
        columns (list): List of column names to chart
        title (str): Title for the page
        pdf (PdfPages): PDF object to add the page to
    """
    plt.figure(figsize=(8.5, 11))
    plt.suptitle(title, fontsize=16, y=0.98)
    
    # Calculate grid dimensions based on number of metrics
    n_metrics = len(columns)
    n_cols = 2  # Use 2 columns
    n_rows = (n_metrics + 1) // 2  # Calculate rows needed
    
    for i, metric in enumerate(columns):
        if metric in df.columns:
            plt.subplot(n_rows, n_cols, i + 1)
            
            # Convert to numeric if possible
            values = pd.to_numeric(df[metric], errors='coerce')
            
            # Create bar chart
            values.plot(kind='bar', color='skyblue')
            plt.title(metric)
            plt.xticks(rotation=45)
            plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for suptitle
    
    pdf.savefig()
    plt.close()

def generate_comparison_report(data_dict, output_path):
    """
    Generate a comprehensive comparison report for multiple stocks
    
    Args:
        data_dict (dict): Dictionary with ticker symbols as keys and data dictionaries as values
        output_path (str): Path to save the report
        
    Returns:
        str: Path to the saved report
    """
    # Create comparison DataFrame
    df = create_comparison_dataframe(data_dict)
    
    # Ensure output directory exists
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Determine file type based on extension
    ext = os.path.splitext(output_path)[1].lower()
    
    if ext == '.xlsx':
        # Save as Excel
        writer = pd.ExcelWriter(output_path, engine='xlsxwriter')
        
        # Main comparison sheet
        df.to_excel(writer, sheet_name='Comparison')
        
        # Add sheet with valuation metrics
        valuation_cols = [col for col in df.columns if any(x in col for x in 
                         ['P/E', 'PEG', 'P/B', 'P/S', 'EV/EBITDA'])]
        if valuation_cols:
            df[valuation_cols].to_excel(writer, sheet_name='Valuation Metrics')
        
        # Add sheet with profitability metrics
        profit_cols = [col for col in df.columns if any(x in col for x in 
                      ['ROE', 'ROA', 'ROIC', 'Margin'])]
        if profit_cols:
            df[profit_cols].to_excel(writer, sheet_name='Profitability')
        
        # Add sheet with technical indicators
        tech_cols = [col for col in df.columns if any(x in col for x in 
                    ['RSI', 'Moving Average', 'MACD', 'Bollinger', 'Volume'])]
        if tech_cols:
            df[tech_cols].to_excel(writer, sheet_name='Technical Indicators')
        
        writer.close()
        return output_path
        
    elif ext == '.csv':
        # Save as CSV
        df.to_csv(output_path)
        return output_path
        
    elif ext == '.pdf':
        # Create a PDF report with charts
        with PdfPages(output_path) as pdf:
            # Title page
            plt.figure(figsize=(8.5, 11))
            plt.text(0.5, 0.5, 'Stock Comparison Report', 
                     ha='center', va='center', fontsize=24)
            plt.text(0.5, 0.45, f'Generated on {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}', 
                     ha='center', va='center', fontsize=14)
            plt.text(0.5, 0.40, f'Comparing: {", ".join(data_dict.keys())}', 
                     ha='center', va='center', fontsize=14)
            plt.axis('off')
            pdf.savefig()
            plt.close()
            
            # Valuation metrics page
            valuation_cols = [col for col in df.columns if any(x in col for x in 
                             ['P/E', 'PEG', 'P/B', 'P/S', 'EV/EBITDA'])]
            if valuation_cols:
                create_metric_charts(df, valuation_cols, 'Valuation Metrics', pdf)
            
            # Profitability metrics page
            profit_cols = [col for col in df.columns if any(x in col for x in 
                          ['ROE', 'ROA', 'ROIC', 'Margin'])]
            if profit_cols:
                create_metric_charts(df, profit_cols, 'Profitability Metrics', pdf)
            
            # Technical indicators page
            tech_cols = [col for col in df.columns if any(x in col for x in 
                        ['RSI', 'Moving Average', 'MACD', 'Bollinger', 'Volume'])]
            if tech_cols:
                create_metric_charts(df, tech_cols, 'Technical Indicators', pdf)
            
            # Create a table with all metrics
            plt.figure(figsize=(8.5, 11))
            plt.suptitle('Comparison Table', fontsize=16, y=0.98)
            
            # Create a table at the center of the page
            ax = plt.subplot(111)
            ax.axis('off')
            tbl = ax.table(
                cellText=df.reset_index().values,
                colLabels=df.reset_index().columns,
                loc='center',
                cellLoc='center',
                colColours=['#f2f2f2']*len(df.reset_index().columns)
            )
            tbl.auto_set_font_size(False)
            tbl.set_fontsize(8)
            tbl.scale(1, 1.5)
            
            pdf.savefig()
            plt.close()
        
        return output_path
    
    else:
        # Default to text file
        with open(output_path, 'w') as f:
            f.write("="*80 + "\n")
            f.write(f"Stock Comparison Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            f.write(f"Comparing: {', '.join(data_dict.keys())}\n\n")
            f.write(df.to_string())
            f.write("\n\n")
            
            # Add sections for different metric groups
            f.write("-"*80 + "\n")
            f.write("VALUATION METRICS\n")
            f.write("-"*80 + "\n")
            valuation_cols = [col for col in df.columns if any(x in col for x in 
                             ['P/E', 'PEG', 'P/B', 'P/S', 'EV/EBITDA'])]
            if valuation_cols:
                f.write(df[valuation_cols].to_string())
                f.write("\n\n")
            
            f.write("-"*80 + "\n")
            f.write("PROFITABILITY METRICS\n")
            f.write("-"*80 + "\n")
            profit_cols = [col for col in df.columns if any(x in col for x in 
                          ['ROE', 'ROA', 'ROIC', 'Margin'])]
            if profit_cols:
                f.write(df[profit_cols].to_string())
                f.write("\n\n")
            
            f.write("-"*80 + "\n")
            f.write("TECHNICAL INDICATORS\n")
            f.write("-"*80 + "\n")
            tech_cols = [col for col in df.columns if any(x in col for x in 
                        ['RSI', 'Moving Average', 'MACD', 'Bollinger', 'Volume'])]
            if tech_cols:
                f.write(df[tech_cols].to_string())
                f.write("\n\n")
        
        return output_path

def rank_stocks(data_dict, metrics=None, ascending=None):
    """
    Rank stocks based on selected metrics
    
    Args:
        data_dict (dict): Dictionary with ticker symbols as keys and data dictionaries as values
        metrics (list, optional): List of metrics to use for ranking. If None, use default metrics.
        ascending (list, optional): List of booleans indicating sort order for each metric.
                                   If None, use default sort orders.
    
    Returns:
        pandas.DataFrame: DataFrame with ranking results
    """
    # Default metrics if none provided
    if metrics is None:
        metrics = ['P/E Ratio', 'PEG Ratio', 'ROE', 'Profit Margin', 'EPS']
    
    # Default sort orders (True = low values are better, False = high values are better)
    if ascending is None:
        ascending = {
            'P/E Ratio': True,     # Lower is better
            'PEG Ratio': True,     # Lower is better
            'P/B Ratio': True,     # Lower is better
            'P/S Ratio': True,     # Lower is better
            'EV/EBITDA': True,     # Lower is better
            'ROE': False,          # Higher is better
            'ROA': False,          # Higher is better
            'ROIC': False,         # Higher is better
            'Profit Margin': False, # Higher is better
            'Operating Margin': False, # Higher is better
            'EPS': False,          # Higher is better
            'RSI': None,           # Middle values (40-60) are better
            'Beta': None           # Depends on risk preference
        }
    
    # Create comparison DataFrame
    df = create_comparison_dataframe(data_dict)
    
    # Calculate rankings for each metric
    rankings = pd.DataFrame(index=df.index)
    rankings['Overall Score'] = 0
    
    for metric in metrics:
        if metric in df.columns:
            # Convert to numeric, replacing non-numeric values with NaN
            values = pd.to_numeric(df[metric], errors='coerce')
            
            # Skip if all values are NaN
            if values.isna().all():
                continue
            
            # Determine sort order
            asc = ascending.get(metric, True)
            
            # Special handling for RSI
            if metric == 'RSI' and asc is None:
                # Calculate distance from ideal RSI (50)
                distance_from_ideal = abs(values - 50)
                rankings[f'{metric} Rank'] = distance_from_ideal.rank()
            else:
                # Rank values (handle NaN by assigning max rank + 1)
                rankings[f'{metric} Rank'] = values.rank(ascending=asc, na_option='bottom')
            
            # Add to overall score (lower rank is better)
            rankings['Overall Score'] += rankings[f'{metric} Rank']
    
    # Calculate overall rank
    rankings['Overall Rank'] = rankings['Overall Score'].rank()
    
    # Sort by overall rank
    rankings = rankings.sort_values('Overall Rank')
    
    # Add original values for reference
    for metric in metrics:
        if metric in df.columns:
            rankings[metric] = df[metric]
    
    return rankings

def create_screener(data_dict, criteria):
    """
    Screen stocks based on specified criteria
    
    Args:
        data_dict (dict): Dictionary with ticker symbols as keys and data dictionaries as values
        criteria (dict): Dictionary of criteria for screening
                        Format: {'metric': ('operator', value)}
                        Operators: '>', '<', '>=', '<=', '==', '!='
    
    Returns:
        pandas.DataFrame: DataFrame with filtered stocks
    """
    # Create comparison DataFrame
    df = create_comparison_dataframe(data_dict)
    
    # Apply each criterion as a filter
    for metric, (operator, value) in criteria.items():
        if metric not in df.columns:
            print(f"Warning: Metric '{metric}' not found, skipping this criterion")
            continue
        
        # Convert column to numeric
        df[metric] = pd.to_numeric(df[metric], errors='coerce')
        
        # Apply filter based on operator
        if operator == '>':
            df = df[df[metric] > value]
        elif operator == '<':
            df = df[df[metric] < value]
        elif operator == '>=':
            df = df[df[metric] >= value]
        elif operator == '<=':
            df = df[df[metric] <= value]
        elif operator == '==':
            df = df[df[metric] == value]
        elif operator == '!=':
            df = df[df[metric] != value]
        else:
            print(f"Warning: Operator '{operator}' not recognized, skipping this criterion")
    
    return df


---
File: /FinanceWebScrapper/src/utils/data_formatter.py
---

"""
Data formatting utilities for the stock scraper
"""
import os
import pandas as pd

def format_data_as_dataframe(data):
    """
    Format the scraped data as a pandas DataFrame
    
    Args:
        data (dict): Dictionary containing scraped data
        
    Returns:
        pandas.DataFrame: DataFrame containing the formatted data
    """
    # Create a DataFrame with a single row
    df = pd.DataFrame([data])
    
    # Group metrics for better display
    grouped_data = {}
    
    # Define metric groups
    metric_groups = {
        "Basic Info": ["Ticker", "Data Timestamp", "Company Name", "Last Updated"],
        "Valuation Ratios": [
            "P/E Ratio", "Forward P/E", "P/B Ratio", "P/S Ratio", 
            "PEG Ratio", "EV/EBITDA", "Trailing P/E"
        ],
        "Profitability": [
            "ROE", "ROIC", "ROA", "Profit Margin", "Operating Margin"
        ],
        "Earnings": [
            "EPS", "EPS Estimate Current Year", "EPS Estimate Next Year",
            "EPS Growth This Year", "EPS Growth Next Year", "EPS Growth Next 5Y",
            "EPS Growth QoQ"
        ],
        "Cash Flow": [
            "Operating Cash Flow", "Capital Expenditures", "Free Cash Flow"
        ],
        "Additional Metrics": [
            "Dividend Yield", "Beta", "Book Value", "Market Cap"
        ],
        "Technical Indicators": [
            "Current Price", "RSI", "BB Middle Band", "BB Upper Band", "BB Lower Band", 
            "BB Width", "BB %B", "BB Signal", "RSI Signal"
        ],
        "Moving Averages": [
            "MA10", "MA20", "MA50", "MA100", "MA200", "EMA12", "EMA26", 
            "EMA50", "EMA200", "MACD"
        ],
        "Volume Indicators": [
            "Current Volume", "Volume MA", "OBV Trend"
        ]
    }
    
    # Initialize the grouped DataFrame
    for group in metric_groups:
        grouped_data[group] = {}
        
    # Sort data into groups
    for key, value in data.items():
        # Skip any error messages
        if isinstance(value, dict) and "error" in value:
            continue
            
        # Skip metadata
        if key in ["Ticker", "Data Timestamp"]:
            grouped_data["Basic Info"][key] = value
            continue
            
        # Determine which group this metric belongs to
        assigned = False
        for group, metrics in metric_groups.items():
            for metric in metrics:
                if metric in key:
                    grouped_data[group][key] = value
                    assigned = True
                    break
            if assigned:
                break
                
        # If not assigned to any group, put it in a misc group
        if not assigned:
            if "Other Metrics" not in grouped_data:
                grouped_data["Other Metrics"] = {}
            grouped_data["Other Metrics"][key] = value
    
    # Convert grouped data to DataFrame
    result_df = pd.DataFrame([data])
    
    return result_df

def save_to_csv(df, file_path):
    """
    Save the DataFrame to a CSV file
    
    Args:
        df (pandas.DataFrame): DataFrame to save
        file_path (str): Path to the output file
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save the DataFrame to CSV
        df.to_csv(file_path, index=False)
        return True
    except Exception as e:
        print(f"Error saving to CSV: {str(e)}")
        return False

def save_to_excel(df, file_path):
    """
    Save the DataFrame to an Excel file with better formatting
    
    Args:
        df (pandas.DataFrame): DataFrame to save
        file_path (str): Path to the output file
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Create a writer
        writer = pd.ExcelWriter(file_path, engine='xlsxwriter')
        
        # Transpose the DataFrame for better readability
        df_t = df.T.reset_index()
        df_t.columns = ['Metric', 'Value']
        
        # Write the DataFrame to Excel
        df_t.to_excel(writer, sheet_name='Metrics', index=False)
        
        # Get the xlsxwriter workbook and worksheet objects
        workbook = writer.book
        worksheet = writer.sheets['Metrics']
        
        # Add some formats
        header_format = workbook.add_format({
            'bold': True,
            'text_wrap': True,
            'valign': 'top',
            'fg_color': '#D7E4BC',
            'border': 1
        })
        
        # Write the column headers with the defined format
        for col_num, value in enumerate(df_t.columns.values):
            worksheet.write(0, col_num, value, header_format)
            
        # Adjust column widths
        worksheet.set_column('A:A', 30)
        worksheet.set_column('B:B', 15)
        
        # Save the workbook
        writer.close()
        return True
    except Exception as e:
        print(f"Error saving to Excel: {str(e)}")
        return False


---
File: /FinanceWebScrapper/src/utils/display_formatter.py
---

"""
Display formatting utilities for the stock scraper results
"""
import pandas as pd
import os

def format_data_for_display(data, max_width=120):
    """
    Format data dictionary into a better organized display format
    
    Args:
        data (dict): Dictionary of financial metrics
        max_width (int): Maximum width for display
        
    Returns:
        str: Formatted string ready for display
    """
    # Group the metrics by category
    categories = {
        "Basic Info": [],
        "Valuation Ratios": [],
        "Profitability Metrics": [],
        "Earnings Metrics": [],
        "Moving Averages": [],
        "Bollinger Bands": [],
        "Momentum Indicators": [],
        "Volume Indicators": [],
        "Other Metrics": []
    }
    
    # Sort data into categories
    for key, value in data.items():
        if any(term in key for term in ["Ticker", "Timestamp", "Updated", "Name", "Sector", "Industry"]):
            categories["Basic Info"].append((key, value))
        elif any(term in key for term in ["P/E", "P/B", "P/S", "EV/EBITDA", "PEG", "Forward P/E"]):
            categories["Valuation Ratios"].append((key, value))
        elif any(term in key for term in ["ROE", "ROA", "ROIC", "Margin"]):
            categories["Profitability Metrics"].append((key, value))
        elif any(term in key for term in ["EPS", "Earnings"]):
            categories["Earnings Metrics"].append((key, value))
        elif any(term in key for term in ["MA", "EMA", "MACD"]):
            categories["Moving Averages"].append((key, value))
        elif any(term in key for term in ["BB ", "Band"]):
            categories["Bollinger Bands"].append((key, value))
        elif any(term in key for term in ["RSI", "Signal"]):
            categories["Momentum Indicators"].append((key, value))
        elif any(term in key for term in ["Volume", "OBV"]):
            categories["Volume Indicators"].append((key, value))
        else:
            categories["Other Metrics"].append((key, value))
    
    # Build the output
    output = []
    
    # Terminal width detection for better formatting
    try:
        terminal_width = os.get_terminal_size().columns
        max_width = min(max_width, terminal_width)
    except (OSError, AttributeError):
        pass  # Use default max_width
    
    # Process each category
    for category, items in categories.items():
        if not items:
            continue
            
        output.append(f"\n{category}:")
        output.append("-" * len(category))
        
        # Create a table for this category
        metric_width = min(60, max_width - 20)
        value_width = max_width - metric_width - 3
        
        # Format as a simple table
        for key, value in sorted(items):
            # Truncate long keys
            if len(key) > metric_width:
                display_key = key[:metric_width-3] + "..."
            else:
                display_key = key
                
            # Format the line
            line = f"{display_key:{metric_width}} | {str(value)[:value_width]}"
            output.append(line)
        
    return "\n".join(output)

def print_grouped_metrics(data):
    """
    Print financial metrics grouped by category
    
    Args:
        data (dict): Dictionary of financial metrics
    """
    try:
        # Try to import tabulate for prettier tables
        from tabulate import tabulate
        
        # Group the metrics by category
        categories = {
            "Basic Info": [],
            "Valuation Ratios": [],
            "Profitability Metrics": [],
            "Earnings Metrics": [],
            "Moving Averages": [],
            "Bollinger Bands": [],
            "Momentum Indicators": [],
            "Volume Indicators": [],
            "Other Metrics": []
        }
        
        # Sort data into categories
        for key, value in data.items():
            if any(term in key for term in ["Ticker", "Timestamp", "Updated", "Name", "Sector", "Industry"]):
                categories["Basic Info"].append((key, value))
            elif any(term in key for term in ["P/E", "P/B", "P/S", "EV/EBITDA", "PEG", "Forward P/E"]):
                categories["Valuation Ratios"].append((key, value))
            elif any(term in key for term in ["ROE", "ROA", "ROIC", "Margin"]):
                categories["Profitability Metrics"].append((key, value))
            elif any(term in key for term in ["EPS", "Earnings"]):
                categories["Earnings Metrics"].append((key, value))
            elif any(term in key for term in ["MA", "EMA", "MACD"]):
                categories["Moving Averages"].append((key, value))
            elif any(term in key for term in ["BB ", "Band"]):
                categories["Bollinger Bands"].append((key, value))
            elif any(term in key for term in ["RSI", "Signal"]):
                categories["Momentum Indicators"].append((key, value))
            elif any(term in key for term in ["Volume", "OBV"]):
                categories["Volume Indicators"].append((key, value))
            else:
                categories["Other Metrics"].append((key, value))
        
        # Process each category
        for category, items in categories.items():
            if not items:
                continue
                
            print(f"\n{category}:")
            print("-" * len(category))
            
            # Create a table for this category
            table_data = []
            for key, value in sorted(items):
                table_data.append([key, value])
            
            # Print as table
            print(tabulate(table_data, headers=["Metric", "Value"], tablefmt="simple"))
    
    except ImportError:
        # Fall back to simple formatting if tabulate is not available
        formatted = format_data_for_display(data)
        print(formatted)

def save_formatted_report(data, filename):
    """
    Save a nicely formatted report to a text file
    
    Args:
        data (dict): Dictionary of financial metrics
        filename (str): Path to output file
    """
    formatted_output = format_data_for_display(data, max_width=100)
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # Write to file
    with open(filename, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write(f"Financial Metrics Report for {data.get('Ticker', 'Unknown')}\n")
        f.write("=" * 80 + "\n")
        f.write(formatted_output)
        f.write("\n\n")
        f.write(f"Generated on: {data.get('Data Timestamp', '')}\n")


---
File: /FinanceWebScrapper/src/utils/email_utils.py
---

"""
Email utilities for sending financial reports
"""
import os
import smtplib
import ssl
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
import logging
from datetime import datetime
import pandas as pd
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

def setup_email_config():
    """
    Set up email configuration
    
    Returns:
        dict: Dictionary with email configuration
    """
    config = {
        "smtp_server": os.environ.get("FINANCE_SMTP_SERVER", "smtp.gmail.com"),
        "smtp_port": int(os.environ.get("FINANCE_SMTP_PORT", 587)),
        "sender_email": os.environ.get("FINANCE_SENDER_EMAIL", ""),
        "sender_password": os.environ.get("FINANCE_SENDER_PASSWORD", ""),
        "use_tls": os.environ.get("FINANCE_USE_TLS", "True").lower() == "true"
    }
    
    return config

def validate_email_config(config):
    """
    Validate email configuration
    
    Args:
        config (dict): Email configuration
        
    Returns:
        bool: True if valid, False otherwise
    """
    required_fields = ["smtp_server", "smtp_port", "sender_email", "sender_password"]
    
    for field in required_fields:
        if not config.get(field):
            logger.warning(f"Missing required email configuration: {field}")
            return False
    
    return True

def parse_email_list(email_string):
    """
    Parse a comma-separated list of email addresses
    
    Args:
        email_string (str): Comma-separated list of email addresses
        
    Returns:
        list: List of email addresses
    """
    if not email_string:
        return []
        
    # Split by comma and strip whitespace
    emails = [email.strip() for email in email_string.split(",")]
    
    # Filter out empty strings
    return [email for email in emails if email]

def send_email(recipients, subject, body, attachment_paths=None, config=None, cc=None, bcc=None):
    """
    Send an email with optional attachments to multiple recipients
    
    Args:
        recipients (str or list): Recipient email address(es)
        subject (str): Email subject
        body (str): Email body
        attachment_paths (list, optional): List of paths to attachment files
        config (dict, optional): Email configuration. If None, will use environment variables.
        cc (str or list, optional): CC email address(es)
        bcc (str or list, optional): BCC email address(es)
        
    Returns:
        bool: True if successful, False otherwise
    """
    if not config:
        config = setup_email_config()
    
    if not validate_email_config(config):
        return False
    
    # Convert string recipient to list if needed
    if isinstance(recipients, str):
        recipients = parse_email_list(recipients)
    
    # Convert CC and BCC to lists if needed
    if isinstance(cc, str):
        cc = parse_email_list(cc)
    elif cc is None:
        cc = []
        
    if isinstance(bcc, str):
        bcc = parse_email_list(bcc)
    elif bcc is None:
        bcc = []
    
    # Ensure we have at least one recipient
    if not recipients and not cc and not bcc:
        logger.error("No recipients specified")
        return False
    
    try:
        # Create message
        message = MIMEMultipart()
        message["From"] = config["sender_email"]
        
        # Set To, CC, and BCC headers
        if recipients:
            message["To"] = ", ".join(recipients)
        if cc:
            message["Cc"] = ", ".join(cc)
        # BCC is not included in headers
        
        message["Subject"] = subject
        
        # Add body
        message.attach(MIMEText(body, "plain"))
        
        # Add attachments if provided
        if attachment_paths:
            for path in attachment_paths:
                if os.path.exists(path):
                    with open(path, "rb") as attachment:
                        part = MIMEApplication(attachment.read(), Name=os.path.basename(path))
                        part["Content-Disposition"] = f'attachment; filename="{os.path.basename(path)}"'
                        message.attach(part)
                else:
                    logger.warning(f"Attachment file not found: {path}")
        
        # Combine all recipients for actual sending
        all_recipients = list(recipients) + list(cc) + list(bcc)
        
        # Connect to server and send email
        if config["use_tls"]:
            context = ssl.create_default_context()
            with smtplib.SMTP(config["smtp_server"], config["smtp_port"]) as server:
                server.starttls(context=context)
                server.login(config["sender_email"], config["sender_password"])
                server.sendmail(config["sender_email"], all_recipients, message.as_string())
        else:
            with smtplib.SMTP(config["smtp_server"], config["smtp_port"]) as server:
                server.login(config["sender_email"], config["sender_password"])
                server.sendmail(config["sender_email"], all_recipients, message.as_string())
        
        logger.info(f"Email sent successfully to {len(all_recipients)} recipients")
        return True
        
    except Exception as e:
        logger.error(f"Error sending email: {str(e)}")
        return False

def generate_metrics_table(data_dict, metrics=None):
    """
    Generate a simple text table of key metrics for multiple stocks
    
    Args:
        data_dict (dict): Dictionary with ticker symbols as keys and data dictionaries as values
        metrics (list, optional): List of metrics to include
        
    Returns:
        str: Formatted table as string
    """
    if not metrics:
        metrics = [
            "P/E Ratio", "Forward P/E", "PEG Ratio", 
            "EPS", "ROE", "Profit Margin", 
            "Current Price", "RSI (14)"
        ]
    
    # Extract data for each ticker
    table_data = {}
    for ticker, data in data_dict.items():
        ticker_data = {"Ticker": ticker}
        
        # Find relevant metrics from different sources
        for metric_name in metrics:
            for key, value in data.items():
                if metric_name in key:
                    ticker_data[metric_name] = value
                    break
        
        table_data[ticker] = ticker_data
    
    # Convert to DataFrame
    if table_data:
        df = pd.DataFrame([v for v in table_data.values()])
        return df.to_string(index=False)
    else:
        return "No data available for metrics table."

def send_consolidated_report(tickers, report_paths, all_data, recipients, summary_path=None, cc=None, bcc=None):
    """
    Send a consolidated report email for multiple stocks
    
    Args:
        tickers (list): List of ticker symbols
        report_paths (dict): Dictionary with ticker symbols as keys and report file paths as values
        all_data (dict): Dictionary with ticker symbols as keys and data dictionaries as values
        recipients (str or list): Recipient email address(es)
        summary_path (str, optional): Path to summary report file
        cc (str or list, optional): CC email address(es)
        bcc (str or list, optional): BCC email address(es)
        
    Returns:
        bool: True if successful, False otherwise
    """
    subject = f"Stock Analysis Report: {', '.join(tickers)} - {datetime.now().strftime('%Y-%m-%d')}"
    
    # Create email body
    body = [
        f"Stock Analysis Report for {len(tickers)} stocks: {', '.join(tickers)}",
        f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "This is an automated report from your Stock Data Scraper application.",
        "",
        "KEY METRICS SUMMARY",
        "===================="
    ]
    
    # Add metrics table
    metrics_table = generate_metrics_table(all_data)
    body.append(metrics_table)
    body.append("")
    
    # Add individual stock highlights
    body.append("INDIVIDUAL STOCK HIGHLIGHTS")
    body.append("=========================")
    
    for ticker, data in all_data.items():
        body.append(f"\n{ticker}:")
        
        # Add key metrics if available
        metrics = [
            ("P/E Ratio", "P/E Ratio"),
            ("Forward P/E", "Forward P/E"),
            ("EPS", "EPS"),
            ("ROE", "ROE"),
            ("Current Price", "Current Price")
        ]
        
        for label, key_prefix in metrics:
            for data_key in data.keys():
                if key_prefix in data_key:
                    body.append(f"  • {label}: {data[data_key]}")
                    break
    
    body.append("\nPlease find the detailed reports attached.")
    
    # Add note about summary report if available
    if summary_path:
        body.append("\nA summary comparison report is also attached.")
    
    body.append("")
    body.append("--")
    body.append("Stock Data Scraper")
    
    # Prepare attachments - include both individual reports and summary
    attachments = list(report_paths.values())
    if summary_path:
        attachments.append(summary_path)
    
    # Debug info
    print(f"Sending consolidated report with {len(attachments)} attachments")
    print(f"Recipients: {recipients}")
    
    # Send email
    return send_email(
        recipients=recipients,
        subject=subject,
        body="\n".join(body),
        attachment_paths=attachments,
        cc=cc,
        bcc=bcc
    )


---
File: /FinanceWebScrapper/src/utils/request_handler.py
---

"""
Request handler module to handle HTTP requests
"""
import logging
import requests
from requests.exceptions import RequestException

logger = logging.getLogger(__name__)

def make_request(url, headers=None, timeout=10, retries=3):
    """
    Make an HTTP GET request with error handling and retries
    
    Args:
        url (str): URL to request
        headers (dict, optional): HTTP headers to send with the request
        timeout (int, optional): Request timeout in seconds
        retries (int, optional): Number of retries for failed requests
        
    Returns:
        requests.Response: Response object
        
    Raises:
        RequestException: If the request fails after all retries
    """
    headers = headers or {}
    attempts = 0
    
    while attempts < retries:
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            
            # Check if the request was successful
            if response.status_code == 200:
                return response
                
            # If we got rate limited, retry after a delay
            if response.status_code == 429:
                logger.warning(f"Rate limited on {url}. Retrying...")
                attempts += 1
                continue
                
            # For other status codes, raise an exception
            response.raise_for_status()
            
        except RequestException as e:
            logger.error(f"Request failed for {url}: {str(e)}")
            attempts += 1
            
            if attempts >= retries:
                raise RequestException(f"Failed to fetch data from {url} after {retries} attempts")
                
    # This should not be reached due to the exception above
    raise RequestException(f"Failed to fetch data from {url}")


---
File: /FinanceWebScrapper/src/__init__.py
---




---
File: /FinanceWebScrapper/src/config.py
---

"""
Configuration settings for the stock scraper application
"""
import os
import logging
from logging.handlers import RotatingFileHandler

# Base directory of the project
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Output directory for CSV files
OUTPUT_DIR = os.path.join(BASE_DIR, 'output')

# Data directory
DATA_DIR = os.path.join(BASE_DIR, 'data')

# Logs directory
LOGS_DIR = os.path.join(BASE_DIR, 'logs')

# Ensure directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)

# User agent for requests
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'

# Request settings
REQUEST_TIMEOUT = 10
REQUEST_RETRIES = 3

# Delay between requests (in seconds)
REQUEST_DELAY = 1

# Configure logging
def setup_logging(log_level=logging.INFO):
    """
    Configure logging for the application
    
    Args:
        log_level (int): Logging level
    """
    # Create logger
    logger = logging.getLogger()
    logger.setLevel(log_level)
    
    # Create console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    
    # Create file handler
    log_file = os.path.join(LOGS_DIR, 'stock_scraper.log')
    file_handler = RotatingFileHandler(
        log_file, maxBytes=1024*1024*5, backupCount=5
    )
    file_handler.setLevel(log_level)
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)
    
    # Add handlers to logger
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# Initialize logging
logger = setup_logging()


---
File: /FinanceWebScrapper/tests/test_yahoo_scraper.py
---

"""
Test module for Yahoo Finance scraper
"""
import unittest
from unittest.mock import patch, MagicMock
from src.scrapers.yahoo_scraper import YahooFinanceScraper

class TestYahooFinanceScraper(unittest.TestCase):
    """Test case for YahooFinanceScraper"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.scraper = YahooFinanceScraper(delay=0)  # No delay for testing
    
    @patch('src.scrapers.yahoo_scraper.make_request')
    def test_scrape_data_success(self, mock_make_request):
        """Test successful data scraping"""
        # Mock the response
        mock_response = MagicMock()
        mock_response.text = """
        <html>
            <body>
                <table>
                    <tr>
                        <td>P/E Ratio (TTM)</td>
                        <td>25.6</td>
                    </tr>
                    <tr>
                        <td>Price/Book (MRQ)</td>
                        <td>15.2</td>
                    </tr>
                    <tr>
                        <td>Price/Sales (TTM)</td>
                        <td>7.9</td>
                    </tr>
                    <tr>
                        <td>Forward P/E</td>
                        <td>22.4</td>
                    </tr>
                </table>
            </body>
        </html>
        """
        mock_make_request.return_value = mock_response
        
        # Call the method
        result = self.scraper._scrape_data('AAPL')
        
        # Verify the results
        self.assertIn('P/E Ratio (Yahoo)', result)
        self.assertEqual(result['P/E Ratio (Yahoo)'], '25.6')
        self.assertIn('P/B Ratio (Yahoo)', result)
        self.assertEqual(result['P/B Ratio (Yahoo)'], '15.2')
        self.assertIn('P/S Ratio (Yahoo)', result)
        self.assertEqual(result['P/S Ratio (Yahoo)'], '7.9')
        self.assertIn('Forward P/E (Yahoo)', result)
        self.assertEqual(result['Forward P/E (Yahoo)'], '22.4')
    
    @patch('src.scrapers.yahoo_scraper.make_request')
    def test_scrape_data_empty(self, mock_make_request):
        """Test scraping with no data found"""
        # Mock the response
        mock_response = MagicMock()
        mock_response.text = """
        <html>
            <body>
                <table>
                    <tr>
                        <td>Some other metric</td>
                        <td>42</td>
                    </tr>
                </table>
            </body>
        </html>
        """
        mock_make_request.return_value = mock_response
        
        # Call the method
        result = self.scraper._scrape_data('AAPL')
        
        # Verify the results
        self.assertEqual(result, {})

if __name__ == '__main__':
    unittest.main()


---
File: /FinanceWebScrapper/main.py
---

#!/usr/bin/env python3
"""
Stock Data Scraper - Main Application Entry Point
"""
import os
import sys
import argparse
from datetime import datetime
import pandas as pd
from dotenv import load_dotenv
import concurrent.futures
import time

# Load environment variables at the start
load_dotenv()

# Add the src directory to the path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), ".")))

from src.scrapers.yahoo_scraper import YahooFinanceScraper
from src.scrapers.finviz_scraper import FinvizScraper
from src.scrapers.google_scraper import GoogleFinanceScraper
from src.scrapers.api_scraper import AlphaVantageAPIScraper, FinhubAPIScraper
from src.indicators.technical_indicators import TechnicalIndicators
from src.utils.data_formatter import format_data_as_dataframe, save_to_csv, save_to_excel
from src.utils.display_formatter import print_grouped_metrics, save_formatted_report
from src.utils.email_utils import send_consolidated_report, parse_email_list
from src.config import setup_logging

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Stock Financial Metrics Scraper")
    parser.add_argument('--tickers', type=str, help="Comma-separated stock ticker symbols to scrape")
    parser.add_argument('--ticker-file', type=str, help="File containing ticker symbols, one per line")
    parser.add_argument('--output-dir', type=str, default="output", help="Directory to save output files")
    parser.add_argument('--sources', type=str, nargs='+', 
                      choices=['yahoo', 'finviz', 'google', 'alphavantage', 'finhub', 'technical', 'all'],
                      default=['all'], help="Data sources to scrape from")
    parser.add_argument('--format', type=str, choices=['csv', 'excel', 'text'], 
                      default='csv', help="Output file format")
    parser.add_argument('--interactive', action='store_true', 
                      help="Run in interactive mode")
    parser.add_argument('--alpha-key', type=str, help="Alpha Vantage API key")
    parser.add_argument('--finhub-key', type=str, help="Finhub API key")
    parser.add_argument('--display-mode', type=str, choices=['table', 'grouped'], 
                      default='grouped', help="How to display results")
    parser.add_argument('--email', type=str, help="Comma-separated email addresses to send the report to")
    parser.add_argument('--cc', type=str, help="Comma-separated email addresses to CC the report to")
    parser.add_argument('--bcc', type=str, help="Comma-separated email addresses to BCC the report to")
    parser.add_argument('--parallel', action='store_true', help="Process tickers in parallel")
    parser.add_argument('--max-workers', type=int, default=4, help="Maximum number of parallel workers")
    parser.add_argument('--delay', type=int, default=1, help="Delay between API requests in seconds")
    parser.add_argument('--summary', action='store_true', help="Generate a summary report for all tickers")
    
    return parser.parse_args()

def get_tickers_interactively():
    """Get ticker symbols from user input"""
    tickers = []
    
    print("\nEnter stock ticker symbols (one per line, type 'done' when finished):")
    while True:
        ticker = input("> ").strip().upper()
        
        if ticker.lower() == 'done':
            if not tickers:
                print("No tickers provided. Please enter at least one ticker.")
                continue
            break
        
        if ticker.lower() == 'quit':
            print("Exiting program.")
            sys.exit(0)
        
        if not ticker:
            print("Please enter a valid ticker symbol.")
            continue
        
        tickers.append(ticker)
        print(f"Added {ticker}. Current tickers: {', '.join(tickers)}")
    
    return tickers

def load_tickers_from_file(file_path):
    """Load ticker symbols from a file"""
    if not os.path.exists(file_path):
        print(f"Error: Ticker file '{file_path}' not found.")
        sys.exit(1)
        
    try:
        with open(file_path, 'r') as f:
            # Read lines, strip whitespace, convert to uppercase, and filter out empty lines
            tickers = [line.strip().upper() for line in f.readlines() if line.strip()]
            
        if not tickers:
            print(f"Error: No valid ticker symbols found in file '{file_path}'.")
            sys.exit(1)
            
        return tickers
    except Exception as e:
        print(f"Error reading ticker file: {str(e)}")
        sys.exit(1)

def run_scrapers(ticker, sources, logger, alpha_key=None, finhub_key=None, delay=1):
    """Run the selected scrapers and combine results"""
    results = {"Ticker": ticker, "Data Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
    
    if 'all' in sources or 'yahoo' in sources:
        logger.info(f"Scraping Yahoo Finance data for {ticker}...")
        print(f"Scraping Yahoo Finance data for {ticker}...")
        yahoo_scraper = YahooFinanceScraper()
        results.update(yahoo_scraper.get_data(ticker))
        time.sleep(delay)  # Add delay to avoid rate limiting
    
    if 'all' in sources or 'finviz' in sources:
        logger.info(f"Scraping Finviz data for {ticker}...")
        print(f"Scraping Finviz data for {ticker}...")
        finviz_scraper = FinvizScraper()
        results.update(finviz_scraper.get_data(ticker))
        time.sleep(delay)  # Add delay to avoid rate limiting
    
    if 'all' in sources or 'google' in sources:
        logger.info(f"Scraping Google Finance data for {ticker}...")
        print(f"Scraping Google Finance data for {ticker}...")
        google_scraper = GoogleFinanceScraper()
        results.update(google_scraper.get_data(ticker))
        time.sleep(delay)  # Add delay to avoid rate limiting
    
    # Alpha Vantage API (only if API key is available)
    if 'all' in sources or 'alphavantage' in sources:
        logger.info(f"Fetching Alpha Vantage API data for {ticker}...")
        print(f"Fetching Alpha Vantage API data for {ticker}...")
        alpha_scraper = AlphaVantageAPIScraper(api_key=alpha_key)
        if alpha_key or os.environ.get("ALPHA_VANTAGE_API_KEY"):
            results.update(alpha_scraper.get_data(ticker))
        else:
            logger.warning("Alpha Vantage API key not provided. Skipping this data source.")
            print("Alpha Vantage API key not provided. Skipping this data source.")
            print("Set with --alpha-key or ALPHA_VANTAGE_API_KEY environment variable.")
        time.sleep(delay)  # Add delay to avoid rate limiting
    
    # Finhub API (only if API key is available)
    if 'all' in sources or 'finhub' in sources:
        logger.info(f"Fetching Finhub API data for {ticker}...")
        print(f"Fetching Finhub API data for {ticker}...")
        finhub_scraper = FinhubAPIScraper(api_key=finhub_key)
        if finhub_key or os.environ.get("FINHUB_API_KEY"):
            results.update(finhub_scraper.get_data(ticker))
        else:
            logger.warning("Finhub API key not provided. Skipping this data source.")
            print("Finhub API key not provided. Skipping this data source.")
            print("Set with --finhub-key or FINHUB_API_KEY environment variable.")
        time.sleep(delay)  # Add delay to avoid rate limiting
    
    # Technical indicators (only if API key is available)
    if 'all' in sources or 'technical' in sources:
        logger.info(f"Calculating technical indicators for {ticker}...")
        print(f"Calculating technical indicators for {ticker}...")
        tech_indicators = TechnicalIndicators(api_key=alpha_key)
        if alpha_key or os.environ.get("ALPHA_VANTAGE_API_KEY"):
            indicator_data = tech_indicators.get_all_indicators(ticker)
            if "error" not in indicator_data:
                # Format the indicator data with source labels
                formatted_indicators = {}
                for key, value in indicator_data.items():
                    if key not in ["Ticker", "Last Updated"]:
                        formatted_indicators[f"{key} (Technical)"] = value
                    else:
                        formatted_indicators[key] = value
                results.update(formatted_indicators)
            else:
                logger.warning(f"Error calculating technical indicators: {indicator_data['error']}")
                print(f"Error calculating technical indicators: {indicator_data['error']}")
        else:
            logger.warning("Alpha Vantage API key not provided. Cannot calculate technical indicators.")
            print("Alpha Vantage API key not provided. Cannot calculate technical indicators.")
            print("Set with --alpha-key or ALPHA_VANTAGE_API_KEY environment variable.")
    
    # Filter out any error messages
    results = {k: v for k, v in results.items() if not isinstance(v, dict) or "error" not in v}
    
    return results

def save_report(data, ticker, file_format, output_dir="output"):
    """
    Save report to file and return the file path
    
    Args:
        data (dict): Stock data dictionary
        ticker (str): Stock ticker symbol
        file_format (str): Output format (csv, excel, text)
        output_dir (str): Directory to save the file
        
    Returns:
        str: Path to the saved file
    """
    # Generate filename
    filename = os.path.join(
        output_dir, 
        f"{ticker}_financial_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    )
    
    # Add extension
    if file_format == 'excel':
        filename += ".xlsx"
    elif file_format == 'text':
        filename += ".txt"
    else:
        filename += ".csv"
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # Save the file in the specified format
    if file_format == 'excel':
        df = format_data_as_dataframe(data)
        save_to_excel(df, filename)
        print(f"Data saved to Excel file: {filename}")
    elif file_format == 'text':
        save_formatted_report(data, filename)
        print(f"Data saved to text report: {filename}")
    else:
        df = format_data_as_dataframe(data)
        save_to_csv(df, filename)
        print(f"Data saved to CSV file: {filename}")
    
    return filename

def send_email_report(data, ticker, recipients, report_path, cc=None, bcc=None):
    """
    Send report via email to multiple recipients
    
    Args:
        data (dict): Stock data dictionary
        ticker (str): Stock ticker symbol
        recipients (str or list): Recipient email address(es)
        report_path (str): Path to the report file
        cc (str or list, optional): CC email address(es)
        bcc (str or list, optional): BCC email address(es)
        
    Returns:
        bool: True if successful, False otherwise
    """
    if isinstance(recipients, str):
        recipients_list = parse_email_list(recipients)
    else:
        recipients_list = recipients
        
    recipient_count = len(recipients_list)
    cc_count = len(parse_email_list(cc)) if cc else 0
    bcc_count = len(parse_email_list(bcc)) if bcc else 0
    
    total_recipients = recipient_count + cc_count + bcc_count
    
    if total_recipients == 0:
        print("No valid email addresses provided.")
        return False
        
    plural = "s" if total_recipients > 1 else ""
    print(f"Sending report to {total_recipients} recipient{plural}...")
    
    # Check if email configuration is set
    if not os.environ.get("FINANCE_SENDER_EMAIL") or not os.environ.get("FINANCE_SENDER_PASSWORD"):
        print("Email configuration not set. Set the following environment variables:")
        print("  - FINANCE_SENDER_EMAIL: Sender email address")
        print("  - FINANCE_SENDER_PASSWORD: Sender email password")
        print("  - FINANCE_SMTP_SERVER: SMTP server (default: smtp.gmail.com)")
        print("  - FINANCE_SMTP_PORT: SMTP port (default: 587)")
        print("  - FINANCE_USE_TLS: Use TLS (default: True)")
        return False
    
    # Send the report
    success = send_stock_report(ticker, recipients_list, report_path, data, cc, bcc)
    
    if success:
        print(f"Report successfully sent to {total_recipients} recipient{plural}")
    else:
        print(f"Failed to send report")
    
    return success

def create_summary_report(all_data, output_dir, file_format):
    """
    Create a summary report for all analyzed tickers
    
    Args:
        all_data (dict): Dictionary with ticker symbols as keys and data dictionaries as values
        output_dir (str): Directory to save the report
        file_format (str): Output format (csv, excel, text)
        
    Returns:
        str: Path to the saved summary file
    """
    # Generate summary data
    tickers = list(all_data.keys())
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Extract key metrics for comparison
    summary_data = []
    
    # Define key metrics to include in summary
    key_metrics = [
        "P/E Ratio", "Forward P/E", "PEG Ratio", "P/B Ratio", "P/S Ratio", 
        "EV/EBITDA", "ROE", "ROA", "ROIC", "Profit Margin", "Operating Margin", 
        "EPS", "Current Price", "RSI (14)", "Beta"
    ]
    
    for ticker, data in all_data.items():
        ticker_summary = {"Ticker": ticker}
        
        # Get the most relevant value for each key metric
        for metric in key_metrics:
            for key, value in data.items():
                if metric in key:
                    ticker_summary[metric] = value
                    break
        
        summary_data.append(ticker_summary)
    
    # Create DataFrame
    summary_df = pd.DataFrame(summary_data)
    
    # Generate filename
    filename = os.path.join(
        output_dir, 
        f"stock_comparison_summary_{timestamp}"
    )
    
    # Add extension
    if file_format == 'excel':
        filename += ".xlsx"
        save_to_excel(summary_df, filename)
    elif file_format == 'text':
        filename += ".txt"
        # Custom text summary
        with open(filename, 'w') as f:
            f.write("="*80 + "\n")
            f.write(f"Stock Comparison Summary - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            f.write(f"Tickers analyzed: {', '.join(tickers)}\n\n")
            f.write(summary_df.to_string(index=False))
            f.write("\n\n")
    else:
        filename += ".csv"
        save_to_csv(summary_df, filename)
    
    print(f"Summary report saved to: {filename}")
    return filename

def process_ticker(ticker, args, logger):
    """
    Process a single ticker
    
    Args:
        ticker (str): Ticker symbol
        args (Namespace): Command line arguments
        logger (Logger): Logger instance
        
    Returns:
        tuple: (ticker, data, report_path)
    """
    print(f"\nProcessing ticker: {ticker}")
    
    # Run scrapers
    data = run_scrapers(ticker, args.sources, logger, 
                     alpha_key=args.alpha_key, 
                     finhub_key=args.finhub_key,
                     delay=args.delay)
    
    # Display results
    print("\n" + "="*80)
    print(f"Financial Metrics for {ticker}")
    print("="*80)
    
    if args.display_mode == 'grouped':
        try:
            print_grouped_metrics(data)
        except ImportError:
            print("Warning: tabulate package not found. Falling back to table display.")
            df = format_data_as_dataframe(data)
            with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                print(df.T)
    else:
        df = format_data_as_dataframe(data)
        # Set display options to show more rows
        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
            print(df.T)  # Transpose for better display
    
    # Save report
    report_path = save_report(data, ticker, args.format, args.output_dir)
    
    return (ticker, data, report_path)

# Update this function in main.py to use consolidated email

def process_all_tickers(tickers, args, logger):
    """
    Process all tickers, either sequentially or in parallel
    
    Args:
        tickers (list): List of ticker symbols
        args (Namespace): Command line arguments
        logger (Logger): Logger instance
        
    Returns:
        dict: Dictionary with ticker symbols as keys and data dictionaries as values
    """
    all_data = {}
    all_reports = {}
    
    if args.parallel and len(tickers) > 1:
        print(f"Processing {len(tickers)} tickers in parallel with {args.max_workers} workers...")
        
        # Process tickers in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            # Submit all tasks
            future_to_ticker = {executor.submit(process_ticker, ticker, args, logger): ticker for ticker in tickers}
            
            # Process results as they complete
            for future in concurrent.futures.as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    ticker, data, report_path = future.result()
                    all_data[ticker] = data
                    all_reports[ticker] = report_path
                except Exception as e:
                    logger.error(f"Error processing ticker {ticker}: {str(e)}")
                    print(f"Error processing ticker {ticker}: {str(e)}")
    else:
        print(f"Processing {len(tickers)} tickers sequentially...")
        
        # Process tickers sequentially
        for ticker in tickers:
            try:
                ticker, data, report_path = process_ticker(ticker, args, logger)
                all_data[ticker] = data
                all_reports[ticker] = report_path
            except Exception as e:
                logger.error(f"Error processing ticker {ticker}: {str(e)}")
                print(f"Error processing ticker {ticker}: {str(e)}")
    
    # Create summary report if requested
    summary_path = None
    if args.summary and len(all_data) > 1:
        summary_path = create_summary_report(all_data, args.output_dir, args.format)
    
    # Send email if requested
    if (args.email or args.cc or args.bcc) and all_reports:
        print("\nPreparing consolidated email report...")
        
        # Import the consolidated email function
        from src.utils.email_utils import send_consolidated_report
        
        # Send consolidated report
        success = send_consolidated_report(
            tickers=list(all_data.keys()),
            report_paths=all_reports,
            all_data=all_data,
            recipients=args.email,
            summary_path=summary_path,
            cc=args.cc,
            bcc=args.bcc
        )
        
        if success:
            print(f"Consolidated report successfully emailed")
        else:
            print(f"Failed to send consolidated report")
    
    return all_data

def main():
    """Main application entry point"""
    # Setup logging
    logger = setup_logging()
    
    print("="*80)
    print("Stock Financial Metrics Scraper")
    print("="*80)
    print("This program collects financial metrics from web sources and APIs:")
    print("  - Web sources: Yahoo Finance, Finviz, Google Finance")
    print("  - APIs: Alpha Vantage, Finhub (API keys required)")
    print("  - Technical indicators: Bollinger Bands, Moving Averages, RSI, Volume indicators")
    print("Added metrics: EV/EBITDA, PEG ratio, ROE, ROIC, EPS, and more!")
    
    args = parse_arguments()
    
    try:
        # Import tabulate only if needed
        if args.display_mode == 'grouped':
            try:
                import tabulate
            except ImportError:
                print("Warning: tabulate package not found. Installing it is recommended for better display...")
                print("Run: pip install tabulate")
                args.display_mode = 'table'
    except:
        pass
    
    # Get tickers to process
    tickers = []
    
    if args.interactive:
        # Inside the main() function, replace the relevant interactive mode section:

        tickers = get_tickers_interactively()
        
        print(f"\nAnalyzing {len(tickers)} ticker(s): {', '.join(tickers)}")
        
        # Process all tickers
        all_data = {}
        all_reports = {}
        
        for ticker in tickers:
            try:
                ticker, data, report_path = process_ticker(ticker, args, logger)
                all_data[ticker] = data
                all_reports[ticker] = report_path
            except Exception as e:
                logger.error(f"Error processing ticker {ticker}: {str(e)}")
                print(f"Error processing ticker {ticker}: {str(e)}")
        
        # Create summary report if requested or if multiple tickers
        summary_path = None
        if len(tickers) > 1:
            create_summary = args.summary
            if not create_summary:
                create_summary = input("\nWould you like to create a summary comparison report? (y/n): ").strip().lower() == 'y'
                
            if create_summary:
                summary_path = create_summary_report(all_data, args.output_dir, args.format)
        
        # Ask if user wants to email the report
        send_email = False
        if not (args.email or args.cc or args.bcc):
            send_email = input("\nWould you like to email these reports? (y/n): ").strip().lower() == 'y'
        else:
            send_email = True
        
        if send_email:
            recipients = args.email
            if not recipients:
                recipients = input("Enter recipient email address(es) (comma-separated): ").strip()
            
            cc = args.cc
            if not cc and input("Would you like to CC anyone? (y/n): ").strip().lower() == 'y':
                cc = input("Enter CC email address(es) (comma-separated): ").strip()
            
            bcc = args.bcc
            if not bcc and input("Would you like to BCC anyone? (y/n): ").strip().lower() == 'y':
                bcc = input("Enter BCC email address(es) (comma-separated): ").strip()
            
            if recipients or cc or bcc:
                from src.utils.email_utils import send_consolidated_report
                print("\nSending consolidated report...")
                
                success = send_consolidated_report(
                    tickers=list(all_data.keys()),
                    report_paths=all_reports,
                    all_data=all_data,
                    recipients=recipients,
                    summary_path=summary_path,
                    cc=cc,
                    bcc=bcc
                )
                
                if success:
                    print("Consolidated report successfully emailed")
                else:
                    print("Failed to send consolidated report")
            else:
                print("No email addresses provided. Skipping email.")
    elif args.ticker_file:
        tickers = load_tickers_from_file(args.ticker_file)
    elif args.tickers:
        tickers = [ticker.strip().upper() for ticker in args.tickers.split(',') if ticker.strip()]
    else:
        print("Error: Please specify tickers using --tickers, --ticker-file, or --interactive")
        sys.exit(1)
    
    if not tickers:
        print("Error: No valid ticker symbols provided.")
        sys.exit(1)
    
    print(f"\nAnalyzing {len(tickers)} ticker(s): {', '.join(tickers)}")
    
    # Process all tickers
    process_all_tickers(tickers, args, logger)
    
    print("\nAll processing complete!")

if __name__ == "__main__":
    main()


---
File: /FinanceWebScrapper/setup.py
---

from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = fh.read().splitlines()

setup(
    name="stock_scraper",
    version="0.1.0",
    author="Your Name",
    author_email="your.email@example.com",
    description="A web scraper for stock financial metrics",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/yourusername/stock-scraper",
    packages=find_packages(),
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.7",
    install_requires=requirements,
    entry_points={
        "console_scripts": [
            "stock-scraper=stock_scraper.main:main",
        ],
    },
)
